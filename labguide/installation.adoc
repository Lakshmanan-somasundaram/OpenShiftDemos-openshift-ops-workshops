## Installation and Verification

The primary method of installing for OpenShift Container Platform is based on Ansible playbooks. These playbooks ship as part of the product in the `openshift-ansible` package.

This method has in the past been referred to as `advanced` installation method and it involves Ansible directly to run the installation playbooks. The advanced installer supports many configuration and customization options. It also covers installation of supporting infrastructure like container-native storage, logging and metrics components.

Your environment comes with a preinstalled cluster that has been deployed using the installer's configuration file (`/etc/ansible/hosts`) when you started the lab.

For more information on installing OpenShift Container Platform, please refer to
the
link:https://docs.openshift.com/container-platform/3.9/install_config/install/quick_install.html[installation
section] of the product documentation.

[NOTE]
====
At this point you should be logged in as `cloud-user` on the OpenShift Master
node via SSH.
====

### Examining the provided Ansible inventory file
First, let's examine the provided installer configuration file.

#### Look at the file
Use `cat`, `less`, or an editor to look at the `/etc/ansible/hosts` file:

----
cat /etc/ansible/hosts
----

General settings and other variable information is defined on lines within the
`[OSEv3:vars]` section. There are also various host groups defined for things
like *Masters* and *Nodes*.

For more details on how OpenShift uses Ansible for its installation, please
refer to the
link:https://docs.openshift.com/container-platform/3.9/install_config/install/advanced_install.html#configuring-ansible[advanced
installer's documentation].

The top-level playbook in `/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml` triggers the installation of the cluster and all of it's components. It's idempotent, which means you can execute this playbook multiple times without harm. This also allows to deploy additional components after the initial install by simply modifying the configuration in `/etc/ansible/hosts` and re-run the installer.
In addition to this there are playbooks that only deploy a specific component or service which makes them faster to execute.

[TIP]
====
A typical multi-host installation like this might normally take around an hour depending on the speed of your internet connection. Disconnected installation options are also available. Prerequisites and other information is all covered in the documentation.
====

### Verifying the Installation
Let's to some basic tests with your installation. As an administrator, most of your interaction with OpenShift will be from the command line. The `oc` program is a command line interface that talks to the OpenShift API.

On the `master` host, after the installation, the `root` system account is
preconfigured to use a special "super administrator" account. It is vitally
important that you protect access to the `root` system account, or to remove
this preconfigured config. Otherwise, anyone who can `sudo` on the master has
super user privileges on the entire cluster.

#### Login on the master
Additionally, your account,`cloud-user`, is preconfigured for cluster administrator access as well.
Type the following command to login as the internal super-user on OpenShift:

----
oc login -u system:admin
----

You will see that you got logged in to a project called 'default'. More on projects later.

----
Logged into "https://master.internal.aws.testdrive.openshift.com:443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-system
    logging
    management-infra
    openshift
    openshift-infra
    openshift-node
    openshift-web-console
    storage

Using project "default".
----

#### Look at the Nodes
Execute the following command to see a list of the *Nodes* that OpenShift knows
about:

----
oc get nodes
----

The output should look something like the following:

----
NAME                          STATUS                     AGE      VERSION
{{INFRA_INTERNAL_FQDN}}     Ready                      1m         v1.9.1+a0ce1bc657
{{MASTER_INTERNAL_FQDN}}    Ready,SchedulingDisabled   1m         v1.9.1+a0ce1bc657
{{NODE1_INTERNAL_FQDN}}    Ready                      1m          v1.9.1+a0ce1bc657
{{NODE2_INTERNAL_FQDN}}    Ready                      1m          v1.9.1+a0ce1bc657
{{NODE3_INTERNAL_FQDN}}    Ready                      1m           v1.9.1+a0ce1bc657
----

All of the systems listed in the `[nodes]` group in the `/etc/ansible/hosts`
file should be listed here. 1 Infrastructure Node, 1 Master and 3 Worker nodes.

The OpenShift *Master* is also a *Node* because it needs to participate in the
software defined network (SDN). By default, however, scheduling is disabled --
no workload will be run on masters. The *Infra* node will only run workloads related to supporting OpenShift infrastructure.

#### Check the Web Console
OpenShift provides a web console for users, developers and application operators
to interact with the environment. There are not many cluster administrative
functions to perform through the web console. Some OpenShift components (like
the internal image registry) run as workload inside the OpenShift environment,
and you could see these things. However, we have not yet explored authentication
topics, so you have no cluster administrator "human" accounts yet.

Point your browser to {{WEB_CONSOLE_URL}} to verify that the web console is
available and responding. You can login using the user `teamuser1` with password `openshift`.
You are not required to do anything in the web console at this point.

WARNING: You will receive a self-signed certificate error in your browser. When
OpenShift is installed, by default, a CA and SSL certificates are generated for
all inter-component communication within OpenShift, including the web console.
It is possible to provide your own SSL certificates during the installation, and
more information can be found in the
link:https://docs.openshift.com/container-platform/3.5/install_config/install/advanced_install.html#advanced-install-custom-certificates[custom
certificates] section of the installation documentation.

#### Verify the Storage cluster
In your environment Red Hat Container-native Storage was installed as part of OpenShift. It will serve robust and persistent storage to both business applications as well as OpenShift infrastructure. It is based on Red Hat Gluster Storage, running on containers on OpenShift nodes and an additional API server called `heketi` that enables the API integration with OpenShift.

We will now use a command line client on the *master* to talk via this server to container storage cluster. It's password protected, so let's export a couple of environment variables first to configure the client:

----
export HEKETI_CLI_SERVER=http://heketi-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY={{HEKETI_ADMIN_PW}}
----

Then use the CLI tool `heketi-cli` to query `heketi` about all the storage clusters it knows about:

----
heketi-cli cluster list
----

`heketi` will list all known clusters with internal UUIDs:

----
Clusters:
ec7a9c8be8327a54839236791bf7ba24 [file][block]<1>
----
<1> This is the internal UUID of the CNS cluster

[NOTE]
====
The cluster UUID will be different for you since it's automatically generated.
====

To get more detailed information about the topology of your CNS cluster (i.e.
nodes, devices and volumes heketi has discovered) run the following command
(output abbreviated):

----
heketi-cli topology info
----

You will get a lengthy output that describes the GlusterFS cluster topology as it is known by `heketi`:

----
Cluster Id: ec7a9c8be8327a54839236791bf7ba24

    File:  true
    Block: true

    Volumes

        Name: heketidbstorage <1>
        Size: 2
        Id: 272c8d37828c62c4002a19027abd2feb
        Cluster Id: ec7a9c8be8327a54839236791bf7ba24
        Mount: {{NODE1_INTERNAL_IP}}:heketidbstorage
        Mount Options: backup-volfile-servers={{NODE2_INTERNAL_IP}},{{NODE2_INTERNAL_IP}}
        Durability Type: replicate
        Replica: 3
        Snapshot: Disabled

    Nodes:

	Node Id: 099b016da11a623bef37de9b85aaa2d7
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 3
	Management Hostname: {{NODE3_INTERNAL_FQDN}}
	Storage Hostname: {{NODE3_INTERNAL_FQDN}}
	Devices:
		Id:e64fac664861c14bd75e3116f805b8fc   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]

	Node Id: 43336d05323e6003be6740dbb7477bd6
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 1
	Management Hostname: {{NODE1_INTERNAL_FQDN}}
	Storage Hostname: {{NODE1_INTERNAL_IP}}
	Devices:
		Id:11a148d8065f6a6220f89c2912d00d13   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]

	Node Id: 6c738028f642e37b2368eca88f8c626c
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 2
	Management Hostname: {{NODE2_INTERNAL_FQDN}}
	Storage Hostname: {{NODE2_INTERNAL_IP}}
	Devices:
		Id:cf7c0dfb258f07be25ac9cd4c4d2e6ae   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]
----
<1> An internal GlusterFS volume that is automatically generated by the setup routine to hold the heketi database.


This output tells you that Red Hat Container-native Storage currently consists a single cluster, which consists of 3 nodes, each with a single block device `/dev/xvdd` of 50GiB in size. The GlusterFS layer will turn these 3 devices/hosts into a single, flat storage pool from which OpenShift will be able to carve out either distinct filesystem volumes or block devices that serve as persistent storage for containers.
